package smg

import (
	"context"
	"errors"
	"fmt"
	"log"
	"net/url"
	"regexp"
	"runtime"
)

// Coordinator manages and assigns tasks to workers.
type Coordinator struct {
	baseURL        *url.URL
	workers        []*Worker
	scrappedUnique map[string]int
	unScrapped     map[int][]*url.URL
	scrapped       map[int][]*url.URL
	skippedURLs    map[string][]string
	errorURLs      map[string]error
	submitCh       chan *coordinatorPayloads
	domainRegex    *regexp.Regexp
	maxDepth       int // -1 means no limit for maxDepth
	interrupted    bool
	processors     []processor
	parallel       int
}

// DataResponse persists information generated by the coordinator
type DataResponse struct {
	BaseURL      *url.URL
	UniqueURLs   map[string]int
	URLsPerDepth map[int][]*url.URL
	SkippedURLs  map[string][]string
	ErrorURLs    map[string]error
	DomainRegex  *regexp.Regexp
	MaxDepth     int // -1 means no limit for maxDepth
	Interrupted  bool
}

// workerPayload contains the urls for the worker
type workerPayload struct {
	currentDepth int
	urls         []*url.URL
}

// coordinatorPayload is the result obtained by a worker
type coordinatorPayload struct {
	depth       int
	sourceURL   *url.URL
	urls        []*url.URL
	invalidURLs []string
	err         error
}

// coordinatorPayloads all the results of the grouped workers
type coordinatorPayloads struct {
	worker string
	got    chan bool
	dumps  []*coordinatorPayload
}

// NewCoordinator returns a coordinator already built
func NewCoordinator(ctx context.Context, baseURL *url.URL, maxDepth int, parallel int) *Coordinator {
	c := &Coordinator{
		baseURL:        baseURL,
		scrappedUnique: make(map[string]int),
		unScrapped:     make(map[int][]*url.URL),
		scrapped:       make(map[int][]*url.URL),
		skippedURLs:    make(map[string][]string),
		errorURLs:      make(map[string]error),
		submitCh:       make(chan *coordinatorPayloads),
		maxDepth:       maxDepth,
		processors: []processor{
			uniqueURLProcessor(),
			errorCheckProcessor(),
			skippedURLProcessor(),
			maxDepthCheckProcessor(),
			domainFilterProcessor(),
		},
	}

	r, _ := regexp.Compile(baseURL.Hostname())
	c.domainRegex = r
	var workers []*Worker
	p := parallel
	if parallel > runtime.NumCPU()*2 || parallel < 0 {
		p = runtime.NumCPU() * 2
	} else if parallel == 0 {
		p = 1
	}
	for i := 0; i < p; i++ {
		w := NewWorker(fmt.Sprintf("Worker %d", i), c.submitCh)
		workers = append(workers, w)
		go w.Run(ctx)
	}
	c.workers = workers
	return c
}

// getAvailableWorkers returns all available workers
func getAvailableWorkers(c *Coordinator) (availableWorkers []*Worker) {
	for _, w := range c.workers {
		if !w.Busy() {
			availableWorkers = append(availableWorkers, w)
		}
	}
	return availableWorkers
}

// sendToWorker sends the work to the worker
func sendToWorker(w *Worker, depth int, urls []*url.URL) {
	w.listenChan <- &workerPayload{
		currentDepth: depth,
		urls:         urls,
	}
}

// distributePayload distributes the work to the available workers
func distributePayload(c *Coordinator, depth int, urls []*url.URL) error {
	availableWorkers := getAvailableWorkers(c)
	if len(availableWorkers) == 0 {
		return errors.New("no workers available")
	}
	if len(urls) <= len(availableWorkers) {
		for i, u := range urls {
			go sendToWorker(availableWorkers[i], depth, []*url.URL{u})
		}
		return nil
	}
	wd := len(urls) / len(availableWorkers)
	i := 0
	for mi, m := range availableWorkers {
		if mi+1 == len(availableWorkers) {
			go sendToWorker(m, depth, urls[i:])
			continue
		}
		go sendToWorker(m, depth, urls[i:i+wd])
		i += wd
	}
	return nil
}

// processMessage processes a message received by the coordinator
func (c *Coordinator) processMessage(wd *coordinatorPayload) {
	c.scrapped[wd.depth-1] = append(c.scrapped[wd.depth-1], wd.sourceURL)
	for _, p := range c.processors {
		r := p.process(c, wd)
		if !r {
			return
		}
	}

	if len(wd.urls) > 0 {
		c.unScrapped[wd.depth] = append(c.unScrapped[wd.depth], wd.urls...)
	}
}

// processMessages processes all messages received from a worker by the coordinator
func (c *Coordinator) processMessages(payloads []*coordinatorPayload) (finished bool) {
	log.Println("processing messages...")
	for _, p := range payloads {
		c.processMessage(p)
	}
	log.Println("processing done...")

	if len(getAvailableWorkers(c)) < 1 {
		log.Println("all workers are busy. deferring payload distribution...")
		return false
	}

	if len(c.unScrapped) > 0 {
		for d, urls := range c.unScrapped {
			err := distributePayload(c, d, urls)
			if err != nil {
				log.Printf("failed to distribute load: %v\n", err)
				break
			}
			delete(c.unScrapped, d)
			log.Printf("distributed payload at depth: %d\n", d)
		}

		return false
	}

	if len(getAvailableWorkers(c)) == len(c.workers) && len(c.unScrapped) == 0 {
		log.Println("scrapping done...")
		return true
	}

	log.Println("no urls to scrape at the moment...")
	return false
}

// Start initiates coordinator to start scraping
func (c *Coordinator) Start(ctx context.Context) {
	log.Printf("Starting coordinator with URL: %s\n", c.baseURL)
	distributePayload(c, 0, []*url.URL{c.baseURL})
	for {
		select {
		case <-ctx.Done():
			log.Println("process interrupted...")
			c.interrupted = true
			return
		case message := <-c.submitCh:
			go func(got chan<- bool) {
				got <- true
			}(message.got)
			log.Printf("got new dump from %s\n", message.worker)
			done := c.processMessages(message.dumps)
			if done {
				log.Println("stopping coordinator...")
				return
			}
		}
	}
}

// Data formats and returns the information generated by the coordinator
func (c *Coordinator) Data() *DataResponse {
	return &DataResponse{
		BaseURL:      c.baseURL,
		UniqueURLs:   c.scrappedUnique,
		URLsPerDepth: c.scrapped,
		SkippedURLs:  c.skippedURLs,
		ErrorURLs:    c.errorURLs,
		DomainRegex:  c.domainRegex,
		MaxDepth:     c.maxDepth,
		Interrupted:  c.interrupted,
	}
}
